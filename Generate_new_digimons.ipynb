{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/death-reaper/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ori_img_path = \"./Digimon\"\n",
    "dest_img_path = \"./resized_digimon_images\"\n",
    "noalpha_img_path = \"./resized_digimon_black/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_images():\n",
    "    \n",
    "    if not os.path.exists(dest_img_path):\n",
    "        os.mkdir(dest_img_path)\n",
    "        \n",
    "    if not os.path.exists(noalpha_img_path):\n",
    "        os.mkdir(noalpha_img_path)\n",
    "        \n",
    "    for digimon_img in os.listdir(ori_img_path):\n",
    "        d_image = cv2.imread(os.path.join(ori_img_path, digimon_img))\n",
    "        if d_image is not None:\n",
    "            d_image = cv2.resize(d_image, (128, 128))\n",
    "            cv2.imwrite(os.path.join(dest_img_path, digimon_img), d_image)\n",
    "            \n",
    "            resized_image = Image.open(os.path.join(dest_img_path, digimon_img))\n",
    "            if resized_image.mode == 'RGBA':\n",
    "                resized_image.load()\n",
    "                bg = Image.new('RGB', resized_image.size, (0, 0, 0))\n",
    "                bg.paste(resized_image, mask=resized_image.split()[3])\n",
    "                bg.save(os.path.join(noalpha_img_path, digimon_img.split('.')[0] + '.jpg'), 'JPEG')\n",
    "            else:\n",
    "                resized_image.convert(\"RGB\")\n",
    "                resized_image.save(os.path.join(noalpha_img_path, digimon_img.split('.')[0] + '.jpg'), 'JPEG')\n",
    "    \n",
    "    print(\"Done preprocessing images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing images\n"
     ]
    }
   ],
   "source": [
    "preprocess_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_tensor(digimon_list):\n",
    "    images_tensor = tf.convert_to_tensor(digimon_list, dtype=tf.string)\n",
    "    image_queue = tf.train.slice_input_producer([images_tensor])\n",
    "    return image_queue\n",
    "    \n",
    "def process_images_to_batches():\n",
    "    \n",
    "    digimon_list = []\n",
    "    for digimon_image in os.listdir(noalpha_img_path):\n",
    "        digimon_list.append(os.path.join(noalpha_img_path, digimon_image))\n",
    "        \n",
    "    image_queue = generate_tensor(digimon_list)\n",
    "    content_file = tf.read_file(image_queue[0])\n",
    "    \n",
    "    decoded_images = tf.image.decode_jpeg(content_file)\n",
    "    decoded_images = tf.image.random_brightness(image = decoded_images, max_delta=0.1)\n",
    "    decoded_images = tf.image.random_flip_left_right(image=decoded_images)\n",
    "    decoded_images = tf.image.random_contrast(decoded_images, lower = 0.1, upper=0.9)\n",
    "    decoded_images = tf.image.resize_images(decoded_images, [128, 128])\n",
    "    decoded_images.set_shape([128, 128, 3])\n",
    "    \n",
    "    decoded_images =tf.cast(decoded_images, dtype=tf.float32)\n",
    "    decoded_images /=255.0\n",
    "    \n",
    "    batches = tf.train.shuffle_batch([decoded_images], \n",
    "                                     64, \n",
    "                                     capacity = 200 + 3*64, \n",
    "                                     num_threads=4, min_after_dequeue = 200)\n",
    "    \n",
    "    \n",
    "    return batches, len(digimon_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(image_input, noise_dim, is_train, reuse=False):\n",
    "    \n",
    "    with tf.variable_scope(\"gen\", reuse = tf.AUTO_REUSE) as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_vaiables()\n",
    "            \n",
    "        gen_weight0 = tf.get_variable(\"gen_weight0\", \n",
    "                                      shape=[noise_dim, 4*4*512],\n",
    "                                      dtype = tf.float32, \n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        gen_bias0 = tf.get_variable(\"gen_bias0\", \n",
    "                                    shape = [4*4*512], \n",
    "                                    dtype = tf.float32,\n",
    "                                    initializer = tf.constant_initializer(0.0))\n",
    "        flat_conv0 = tf.add(tf.matmul(image_input, gen_weight0), gen_bias0, name = \"flat_conv0\")\n",
    "        \n",
    "        ## 4*4*512\n",
    "        gen_conv1 = tf.reshape(flat_conv0, [-1, 4, 4, 512], name = \"gen_conv1\")\n",
    "        gen_bias1 = tf.contrib.layers.batch_norm(gen_conv1, \n",
    "                                                 epsilon = 1e-5, \n",
    "                                                 updates_collections=None, \n",
    "                                                 is_training=is_train, \n",
    "                                                 decay = 0.9, \n",
    "                                                 scope=\"gen_bias1\", \n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "        \n",
    "        ## 8*8*256\n",
    "        gen_conv2 = tf.layers.conv2d_transpose(gen_bias1, \n",
    "                                               256, \n",
    "                                               kernel_size = [5, 5], \n",
    "                                               strides = [2, 2], \n",
    "                                               padding = \"SAME\", \n",
    "                                               kernel_initializer = tf.truncated_normal_initializer(stddev=0.02), \n",
    "                                               name = \"gen_conv2\")\n",
    "        gen_bias2 = tf.contrib.layers.batch_norm(gen_conv2, \n",
    "                                                 decay = 0.9, \n",
    "                                                 epsilon = 1e-5, \n",
    "                                                 updates_collections = None, \n",
    "                                                 is_training = is_train, \n",
    "                                                 scope = \"gen_bias2\", \n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "        \n",
    "        ## 16*16*128\n",
    "        gen_conv3 = tf.layers.conv2d_transpose(gen_bias2, \n",
    "                                               128, \n",
    "                                               kernel_size = [5, 5], \n",
    "                                               strides = [2, 2], \n",
    "                                               padding = \"SAME\", \n",
    "                                               kernel_initializer = tf.truncated_normal_initializer(stddev=0.02), \n",
    "                                               name = \"gen_conv3\")\n",
    "        gen_bias3 = tf.contrib.layers.batch_norm(gen_conv3, \n",
    "                                                 decay = 0.9, \n",
    "                                                 epsilon = 1e-5, \n",
    "                                                 updates_collections = None, \n",
    "                                                 is_training=is_train, \n",
    "                                                 scope=\"gen_bias3\", \n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "        \n",
    "        \n",
    "        ## 32*32*64\n",
    "        gen_conv4 = tf.layers.conv2d_transpose(gen_bias3, \n",
    "                                               64, \n",
    "                                               kernel_size = [5, 5], \n",
    "                                               strides = [2, 2], \n",
    "                                               padding = \"SAME\", \n",
    "                                               kernel_initializer = tf.truncated_normal_initializer(stddev=0.02), \n",
    "                                               name = \"gen_conv4\")\n",
    "        gen_bias4 = tf.contrib.layers.batch_norm(gen_conv4, \n",
    "                                                 decay = 0.9, \n",
    "                                                 epsilon = 1e-5, \n",
    "                                                 updates_collections = None, \n",
    "                                                 is_training=is_train, \n",
    "                                                 scope=\"gen_bias4\", \n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "        \n",
    "        ## 64*64*32\n",
    "        gen_conv5 = tf.layers.conv2d_transpose(gen_bias4, \n",
    "                                               32, \n",
    "                                               kernel_size = [5, 5], \n",
    "                                               strides = [2, 2], \n",
    "                                               padding = \"SAME\", \n",
    "                                               kernel_initializer = tf.truncated_normal_initializer(stddev=0.02), \n",
    "                                               name = \"gen_conv5\")\n",
    "        gen_bias5 = tf.contrib.layers.batch_norm(gen_conv5, \n",
    "                                                 decay = 0.9, \n",
    "                                                 epsilon = 1e-5, \n",
    "                                                 updates_collections = None, \n",
    "                                                 is_training=is_train, \n",
    "                                                 scope=\"gen_bias5\", \n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "        \n",
    "        ## 128*128*3\n",
    "        gen_conv6 = tf.layers.conv2d_transpose(gen_bias5, \n",
    "                                               3, \n",
    "                                               kernel_size = [5, 5], \n",
    "                                               strides = [2, 2], \n",
    "                                               padding = \"SAME\", \n",
    "                                               kernel_initializer = tf.truncated_normal_initializer(stddev=0.02), \n",
    "                                               name = \"gen_conv6\")\n",
    "        gen_bias6 = tf.contrib.layers.batch_norm(gen_conv6, \n",
    "                                                 decay = 0.9, \n",
    "                                                 epsilon = 1e-5, \n",
    "                                                 updates_collections = None, \n",
    "                                                 is_training=is_train, \n",
    "                                                 scope=\"gen_bias6\", \n",
    "                                                 activation_fn = tf.nn.tanh)\n",
    "        \n",
    "        return gen_bias6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(input_image, is_train, reuse = False):\n",
    "    with tf.variable_scope(\"disc\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "            \n",
    "        ## 64*64*64\n",
    "        disc_conv0 = tf.layers.conv2d(input_image, \n",
    "                                      64, \n",
    "                                      kernel_size = [5, 5], \n",
    "                                      strides = [2, 2], \n",
    "                                      padding = \"SAME\", \n",
    "                                      kernel_initializer = tf.truncated_normal_initializer(stddev = 0.02), \n",
    "                                      name = \"disc_conv0\")\n",
    "        \n",
    "        disc_bias0 = tf.contrib.layers.batch_norm(disc_conv0, \n",
    "                                                  decay = 0.9, \n",
    "                                                  epsilon = 1e-5, \n",
    "                                                  updates_collections = None, \n",
    "                                                  is_training = is_train, \n",
    "                                                  scope = \"disc_bias0\", \n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "        \n",
    "        ## 32*32*128\n",
    "        disc_conv1 = tf.layers.conv2d(disc_bias0, \n",
    "                                      128, \n",
    "                                      kernel_size = [5, 5], \n",
    "                                      strides = [2, 2], \n",
    "                                      padding = \"SAME\", \n",
    "                                      kernel_initializer = tf.truncated_normal_initializer(stddev = 0.02), \n",
    "                                      name = \"disc_conv1\")\n",
    "        \n",
    "        disc_bias1 = tf.contrib.layers.batch_norm(disc_conv1, \n",
    "                                                  decay = 0.9, \n",
    "                                                  epsilon = 1e-5, \n",
    "                                                  updates_collections = None, \n",
    "                                                  is_training = is_train, \n",
    "                                                  scope = \"disc_bias1\", \n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "        \n",
    "        ## 16*16*256\n",
    "        disc_conv2 = tf.layers.conv2d(disc_bias1, \n",
    "                                      256, \n",
    "                                      kernel_size = [5, 5], \n",
    "                                      strides = [2, 2], \n",
    "                                      padding = \"SAME\", \n",
    "                                      kernel_initializer = tf.truncated_normal_initializer(stddev = 0.02), \n",
    "                                      name = \"disc_conv2\")\n",
    "        \n",
    "        disc_bias2 = tf.contrib.layers.batch_norm(disc_conv2, \n",
    "                                                  decay = 0.9, \n",
    "                                                  epsilon = 1e-5, \n",
    "                                                  updates_collections = None, \n",
    "                                                  is_training = is_train, \n",
    "                                                  scope = \"disc_bias2\", \n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "        \n",
    "        ## 8*8*512\n",
    "        disc_conv3 = tf.layers.conv2d(disc_bias2, \n",
    "                                      512, \n",
    "                                      kernel_size = [5, 5], \n",
    "                                      strides = [2, 2], \n",
    "                                      padding = \"SAME\", \n",
    "                                      kernel_initializer = tf.truncated_normal_initializer(stddev = 0.02), \n",
    "                                      name = \"disc_conv3\")\n",
    "        \n",
    "        disc_bias3 = tf.contrib.layers.batch_norm(disc_conv3, \n",
    "                                                  decay = 0.9, \n",
    "                                                  epsilon = 1e-5, \n",
    "                                                  updates_collections = None, \n",
    "                                                  is_training = is_train, \n",
    "                                                  scope = \"disc_bias3\", \n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "        \n",
    "        ## 4*4*1024\n",
    "        disc_conv4 = tf.layers.conv2d(disc_bias3, \n",
    "                                      1024, \n",
    "                                      kernel_size = [5, 5], \n",
    "                                      strides = [2, 2], \n",
    "                                      padding = \"SAME\", \n",
    "                                      kernel_initializer = tf.truncated_normal_initializer(stddev = 0.02), \n",
    "                                      name = \"disc_conv4\")\n",
    "        \n",
    "        disc_bias4 = tf.contrib.layers.batch_norm(disc_conv4, \n",
    "                                                  decay = 0.9, \n",
    "                                                  epsilon = 1e-5, \n",
    "                                                  updates_collections = None, \n",
    "                                                  is_training = is_train, \n",
    "                                                  scope = \"disc_bias4\", \n",
    "                                                 activation_fn = tf.nn.relu)\n",
    "        \n",
    "        \n",
    "        ## zeroth fully connected layer\n",
    "        dimen = int(np.prod(disc_bias4.get_shape()[1:]))\n",
    "        disc_fc0 = tf.reshape(disc_bias4, shape=[-1, dimen], name = \"disc_fc0\")\n",
    "        \n",
    "        disc_fc_weight0 = tf.get_variable(\"disc_fc_weight0\", \n",
    "                                       shape=[disc_fc0.shape[-1], 1024], \n",
    "                                       dtype=tf.float32, \n",
    "                                       initializer = tf.truncated_normal_initializer(stddev=0.02))\n",
    "        disc_fc_bias0 = tf.get_variable(\"disc_fc_bias0\", \n",
    "                                        shape=[1024], \n",
    "                                        dtype=tf.float32, \n",
    "                                        initializer = tf.constant_initializer(0.0))\n",
    "        disc_fc_act0 = tf.nn.relu(tf.matmul(disc_fc0, disc_fc_weight0) + disc_fc_bias0)\n",
    "        \n",
    "        ## first fully connected layer        \n",
    "        disc_fc_weight1 = tf.get_variable(\"disc_fc_weight1\", \n",
    "                                       shape=[1024, 128], \n",
    "                                       dtype=tf.float32, \n",
    "                                       initializer = tf.truncated_normal_initializer(stddev=0.02))\n",
    "        disc_fc_bias1 = tf.get_variable(\"disc_fc_bias1\", \n",
    "                                        shape=[128], \n",
    "                                        dtype=tf.float32, \n",
    "                                        initializer = tf.constant_initializer(0.0))\n",
    "        disc_fc_act1 = tf.nn.relu(tf.matmul(disc_fc_act0, disc_fc_weight1) + disc_fc_bias1)\n",
    "        \n",
    "        ## second fully connected layer        \n",
    "        disc_fc_weight2 = tf.get_variable(\"disc_fc_weight2\", \n",
    "                                       shape=[128, 1], \n",
    "                                       dtype=tf.float32, \n",
    "                                       initializer = tf.truncated_normal_initializer(stddev=0.02))\n",
    "        disc_fc_bias2 = tf.get_variable(\"disc_fc_bias2\", \n",
    "                                        shape=[1], \n",
    "                                        dtype=tf.float32, \n",
    "                                        initializer = tf.constant_initializer(0.0))\n",
    "        disc_fc_act2 = tf.matmul(disc_fc_act1, disc_fc_weight2) + disc_fc_bias2\n",
    "        \n",
    "        return disc_fc_act2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_digiGan():\n",
    "    tf_sess = tf.Session()\n",
    "    batches = 64\n",
    "    noise_dim = 100\n",
    "    \n",
    "    real_image = tf.placeholder(tf.float32, shape = [None, 128, 128, 3], name = \"real_image\")\n",
    "    noise_input = tf.placeholder(tf.float32, shape = [None, noise_dim], name = \"noise_input\")\n",
    "    is_train = tf.placeholder(tf.bool, name = \"is_train\")\n",
    "    \n",
    "    gen_func = generator(noise_input, noise_dim, is_train)\n",
    "    disc_func = discriminator(real_image, is_train)\n",
    "    disc_gen_func = discriminator(gen_func, is_train, reuse = True)\n",
    "    \n",
    "    gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_gen_func, \n",
    "                                                                      labels = tf.ones_like(disc_gen_func)))\n",
    "    \n",
    "    disc_X_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_func, \n",
    "                                                                         labels = tf.fill([batches, 1], 0.9)))\n",
    "    disc_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_gen_func, \n",
    "                                                                           labels=tf.zeros_like(disc_gen_func)))\n",
    "    disc_loss = disc_X_loss+disc_fake_loss\n",
    "    \n",
    "    trainable_vars = tf.trainable_variables()\n",
    "    \n",
    "    g_vars = [var for var in trainable_vars if 'gen' in var.name]\n",
    "    d_vars = [var for var in trainable_vars if 'disc' in var.name]\n",
    "    \n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse = tf.AUTO_REUSE):\n",
    "        disc_train_X = tf.train.AdamOptimizer(0.001).minimize(disc_X_loss, var_list = d_vars)\n",
    "        disc_train_fake = tf.train.AdamOptimizer(0.001).minimize(disc_fake_loss, var_list=d_vars)\n",
    "        gen_train = tf.train.AdamOptimizer(0.001).minimize(gen_loss, var_list=g_vars)\n",
    "        \n",
    "    disc_count_real = tf.placeholder(tf.float32)\n",
    "    disc_count_fake = tf.placeholder(tf.float32)\n",
    "    gen_count = tf.placeholder(tf.float32)\n",
    "    disc_after_train_fake = tf.reduce_mean(discriminator(generator(noise_input, noise_dim, is_train), is_train))\n",
    "    disc_after_train_real = tf.reduce_mean(discriminator(real_image, is_train))\n",
    "    images = generator(noise_input, noise_dim, is_train)\n",
    "\n",
    "    tf.summary.scalar(\"Generator_loss\", gen_loss)\n",
    "    tf.summary.scalar(\"Discriminator_fake_loss\", disc_fake_loss)\n",
    "    tf.summary.scalar(\"Discriminator_real_loss\", disc_X_loss)\n",
    "    tf.summary.scalar(\"discriminator_count_real\", disc_count_real)\n",
    "    tf.summary.scalar(\"discriminator_count_fake\", disc_count_fake)\n",
    "    tf.summary.scalar(\"generator_count\", gen_count)\n",
    "    tf.summary.scalar(\"discriminator_values_of_generated\", disc_after_train_fake)\n",
    "    tf.summary.scalar(\"discriminator_values_of_real\", disc_after_train_real)\n",
    "    tf.summary.image(\"generated_fake_images\", images)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    logdir = \"tensorboard/digimonGan/\"\n",
    "    fileWriter = tf.summary.FileWriter(logdir, graph = tf_sess.graph)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    tf_sess.run(tf.global_variables_initializer())\n",
    "    tf_sess.run(tf.local_variables_initializer())\n",
    "    gen_loss = 0.0\n",
    "    discLossReal, discLossFake = 1.0, 1.0\n",
    "    discFakeCount, discRealCount, genCount = 0.0, 0.0, 0.0\n",
    "    \n",
    "    images_batch, images_num = process_images_to_batches()\n",
    "    tot_batches = int(images_num/batches)\n",
    "    \n",
    "    print(logdir + \"\\n\\n\")\n",
    "    print(merged + \"\\n\\n\")\n",
    "    print(str(disc_X_loss) + \"\\n\\n\")\n",
    "    print(str(disc_fake_loss) + \"\\n\\n\")\n",
    "    print(str(disc_loss) + \"\\n\\n\")\n",
    "    print(str(trainable_vars) + \"\\n\\n\")\n",
    "    print(str(gen_func) + \"\\n\\n\")\n",
    "    print(str(disc_gen_func) + \"\\n\\n\")\n",
    "    print(str(d_vars) + \"\\n\\n\")\n",
    "    print(str(g_vars) + \"\\n\\n\")\n",
    "    print(str(disc_after_train_fake) + \"\\n\\n\")\n",
    "    print(str(disc_after_train_real) + \"\\n\\n\")\n",
    "    print(\"total no. of batches: \" + str(tot_batches) + \"\\n\\n\")\n",
    "    print(\"total no. of images: \" + str(images_num)+ \"\\n\\n\")\n",
    "    \n",
    "    print(\"Training started......\\n\\n\")\n",
    "    for i in range(5000):\n",
    "        \n",
    "        for batch in range(tot_batches):\n",
    "            d_iters = 5\n",
    "            g_iters = 1\n",
    "            noiseInp = np.random.uniform(-1.0, 1.0, size=[batches, noise_dim]).astype(np.float32)\n",
    "            \n",
    "            for j in range(d_iters):\n",
    "                ori_batch = tf_sess.run(images_batch)\n",
    "                \n",
    "                if discLossReal > 0.45:\n",
    "                    _, discLossReal, discLossFake, genLoss = tf_sess.run([disc_train_X, \n",
    "                                                              disc_X_loss, \n",
    "                                                              disc_fake_loss, \n",
    "                                                              gen_loss], \n",
    "                                                              feed_dict = {real_image: ori_batch, \n",
    "                                                                           is_train: True})\n",
    "                    discRealCount += 1\n",
    "                \n",
    "                if discLossFake > 0.6:\n",
    "                    _, discLossReal, discLossFake, genLoss = tf_sess.run([disc_train_fake, \n",
    "                                                                  disc_X_loss, \n",
    "                                                                  disc_fake_loss, \n",
    "                                                                  gen_loss], \n",
    "                                                                  feed_dict = {real_image: ori_batch, \n",
    "                                                                               is_train: True, \n",
    "                                                                               noise_input: noiseInp})\n",
    "                    discFakeCount += 1\n",
    "                    \n",
    "            for j in range(g_iters):\n",
    "                if genLoss > 0.5:\n",
    "                    _, discLossReal, discLossFake, genLoss = tf_sess.run([gen_train, \n",
    "                                                                          disc_X_loss, \n",
    "                                                                          disc_fake_loss, \n",
    "                                                                          gen_loss], \n",
    "                                                                          feed_dict={real_image: ori_batch, \n",
    "                                                                                     is_train: True, \n",
    "                                                                                     noise_input: noiseInp})\n",
    "                    genCount += 1\n",
    "                    \n",
    "        if i%50 == 0:\n",
    "            if not os.path.exists(\"./digimonModels\"):\n",
    "                os.mkdir(\"./digimonModels\")\n",
    "                \n",
    "            sample_noise = np.random.uniform(-1.0, 1.0, size = [batches, noise_dim]).astype(np.float32)\n",
    "            test_img = tf_sess.run(gen_func, feed_dict = {noise_input: sample_noise, is_train: False})\n",
    "            \n",
    "            saved_at = saver.save(tf_sess, \"./digimonModels/pretrained_digi_gan.ckpt\", global_step = i)\n",
    "            print(\"Image saved at: {0}\".format(saved_at))\n",
    "            print(\"gen_loss: {0}, \\ndiscLossReal: {1}, \\ndiscLossFake: {2}\".format(genLoss, discLossReal, discLossFake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's generate some digimons\n",
      "tensorboard/digimonGan/\n",
      "\n",
      "\n",
      "Tensor(\"add_1:0\", shape=(), dtype=string)\n",
      "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "Tensor(\"add:0\", shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "[<tf.Variable 'gen/gen_weight0:0' shape=(100, 8192) dtype=float32_ref>, <tf.Variable 'gen/gen_bias0:0' shape=(8192,) dtype=float32_ref>, <tf.Variable 'gen/gen_bias1/beta:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'gen/gen_conv2/kernel:0' shape=(5, 5, 256, 512) dtype=float32_ref>, <tf.Variable 'gen/gen_conv2/bias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'gen/gen_bias2/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'gen/gen_conv3/kernel:0' shape=(5, 5, 128, 256) dtype=float32_ref>, <tf.Variable 'gen/gen_conv3/bias:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'gen/gen_bias3/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'gen/gen_conv4/kernel:0' shape=(5, 5, 64, 128) dtype=float32_ref>, <tf.Variable 'gen/gen_conv4/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'gen/gen_bias4/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'gen/gen_conv5/kernel:0' shape=(5, 5, 32, 64) dtype=float32_ref>, <tf.Variable 'gen/gen_conv5/bias:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'gen/gen_bias5/beta:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'gen/gen_conv6/kernel:0' shape=(5, 5, 3, 32) dtype=float32_ref>, <tf.Variable 'gen/gen_conv6/bias:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'gen/gen_bias6/beta:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'disc/disc_conv0/kernel:0' shape=(5, 5, 3, 64) dtype=float32_ref>, <tf.Variable 'disc/disc_conv0/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'disc/disc_bias0/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'disc/disc_conv1/kernel:0' shape=(5, 5, 64, 128) dtype=float32_ref>, <tf.Variable 'disc/disc_conv1/bias:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'disc/disc_bias1/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'disc/disc_conv2/kernel:0' shape=(5, 5, 128, 256) dtype=float32_ref>, <tf.Variable 'disc/disc_conv2/bias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'disc/disc_bias2/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'disc/disc_conv3/kernel:0' shape=(5, 5, 256, 512) dtype=float32_ref>, <tf.Variable 'disc/disc_conv3/bias:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'disc/disc_bias3/beta:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'disc/disc_conv4/kernel:0' shape=(5, 5, 512, 1024) dtype=float32_ref>, <tf.Variable 'disc/disc_conv4/bias:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'disc/disc_bias4/beta:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'disc/disc_fc_weight0:0' shape=(16384, 1024) dtype=float32_ref>, <tf.Variable 'disc/disc_fc_bias0:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'disc/disc_fc_weight1:0' shape=(1024, 128) dtype=float32_ref>, <tf.Variable 'disc/disc_fc_bias1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'disc/disc_fc_weight2:0' shape=(128, 1) dtype=float32_ref>, <tf.Variable 'disc/disc_fc_bias2:0' shape=(1,) dtype=float32_ref>]\n",
      "\n",
      "\n",
      "Tensor(\"gen/gen_bias6/Tanh:0\", shape=(?, 128, 128, 3), dtype=float32)\n",
      "\n",
      "\n",
      "Tensor(\"disc_1/add_2:0\", shape=(?, 1), dtype=float32)\n",
      "\n",
      "\n",
      "[<tf.Variable 'disc/disc_conv0/kernel:0' shape=(5, 5, 3, 64) dtype=float32_ref>, <tf.Variable 'disc/disc_conv0/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'disc/disc_bias0/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'disc/disc_conv1/kernel:0' shape=(5, 5, 64, 128) dtype=float32_ref>, <tf.Variable 'disc/disc_conv1/bias:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'disc/disc_bias1/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'disc/disc_conv2/kernel:0' shape=(5, 5, 128, 256) dtype=float32_ref>, <tf.Variable 'disc/disc_conv2/bias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'disc/disc_bias2/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'disc/disc_conv3/kernel:0' shape=(5, 5, 256, 512) dtype=float32_ref>, <tf.Variable 'disc/disc_conv3/bias:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'disc/disc_bias3/beta:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'disc/disc_conv4/kernel:0' shape=(5, 5, 512, 1024) dtype=float32_ref>, <tf.Variable 'disc/disc_conv4/bias:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'disc/disc_bias4/beta:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'disc/disc_fc_weight0:0' shape=(16384, 1024) dtype=float32_ref>, <tf.Variable 'disc/disc_fc_bias0:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'disc/disc_fc_weight1:0' shape=(1024, 128) dtype=float32_ref>, <tf.Variable 'disc/disc_fc_bias1:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'disc/disc_fc_weight2:0' shape=(128, 1) dtype=float32_ref>, <tf.Variable 'disc/disc_fc_bias2:0' shape=(1,) dtype=float32_ref>]\n",
      "\n",
      "\n",
      "[<tf.Variable 'gen/gen_weight0:0' shape=(100, 8192) dtype=float32_ref>, <tf.Variable 'gen/gen_bias0:0' shape=(8192,) dtype=float32_ref>, <tf.Variable 'gen/gen_bias1/beta:0' shape=(512,) dtype=float32_ref>, <tf.Variable 'gen/gen_conv2/kernel:0' shape=(5, 5, 256, 512) dtype=float32_ref>, <tf.Variable 'gen/gen_conv2/bias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'gen/gen_bias2/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'gen/gen_conv3/kernel:0' shape=(5, 5, 128, 256) dtype=float32_ref>, <tf.Variable 'gen/gen_conv3/bias:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'gen/gen_bias3/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'gen/gen_conv4/kernel:0' shape=(5, 5, 64, 128) dtype=float32_ref>, <tf.Variable 'gen/gen_conv4/bias:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'gen/gen_bias4/beta:0' shape=(64,) dtype=float32_ref>, <tf.Variable 'gen/gen_conv5/kernel:0' shape=(5, 5, 32, 64) dtype=float32_ref>, <tf.Variable 'gen/gen_conv5/bias:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'gen/gen_bias5/beta:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'gen/gen_conv6/kernel:0' shape=(5, 5, 3, 32) dtype=float32_ref>, <tf.Variable 'gen/gen_conv6/bias:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'gen/gen_bias6/beta:0' shape=(3,) dtype=float32_ref>]\n",
      "\n",
      "\n",
      "Tensor(\"Mean_3:0\", shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "Tensor(\"Mean_4:0\", shape=(), dtype=float32)\n",
      "\n",
      "\n",
      "total no. of batches: 16\n",
      "\n",
      "\n",
      "total no. of images: 1063\n",
      "\n",
      "\n",
      "Training started......\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's generate some digimons\")\n",
    "\n",
    "train_digiGan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
